{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"machine_learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_predictions_df = pd.read_csv(f\"eval_outputs/{dataset_name}_gpt4o_results.csv\")\n",
    "llama_predictions_df = pd.read_csv(f\"eval_outputs/{dataset_name}_llama_vanilla_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(gpt_predictions_df['Question'] == llama_predictions_df['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_gpt = gpt_predictions_df['Predicted Answer'] == gpt_predictions_df['Correct Answer']\n",
    "incorrect_llama = llama_predictions_df['Predicted Answer'] != llama_predictions_df['Correct Answer']\n",
    "interesting_questions = gpt_predictions_df[correct_gpt & incorrect_llama]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_revist = interesting_questions['Question'].tolist()\n",
    "answers_of_interest = interesting_questions['Correct Answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = .5\n",
    "number_of_examples_per_question = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example(question, answer, prev_examples, temperature=0.5):\n",
    "    system_prompt = f'''You are an advanced AI model tasked with assisting in creating training data for smaller AI models. \n",
    "\n",
    "    The task of the smaller model is to take in a question in the field of {dataset_name} and a list of possible answers, and produce an answer that is most likely to be correct, without providing any explanation or reasoning.\n",
    "\n",
    "    When given a question, you should:\n",
    "\n",
    "    1. Analyze the knowledge or reasoning required to answer it correctly.\n",
    "\n",
    "    2. Generate a new Q&A pair that tests the same knowledge or reasoning skills.\n",
    "\n",
    "    The objective is that the smaller model should be able to answer the original question correctly if it can answer the new question correctly.\n",
    "\n",
    "    Ensure that:\n",
    "\n",
    "    - The information present in the original question is also present in the new question.\n",
    "    - The new question has exactly 4 answer options (A, B, C, D). Mention these options in the question.\n",
    "    - The correct answer is clearly indicated in the format shown below.\n",
    "\n",
    "    Use the following format strictly:\n",
    "    ```\n",
    "    <question>Question text with options</question>\n",
    "    <answer>Correct answer (e.g., A or B)</answer>\n",
    "    ```\n",
    "\n",
    "    Do not include explanations, reasoning, or any additional text.'''\n",
    "    \n",
    "    system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
    "    messages = [system_message]\n",
    "    \n",
    "    messages.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f'''Here is a question that was correctly answered by a large model but incorrectly answered by a smaller model:\n",
    "    {question} and its answer: {answer}\n",
    "\n",
    "    Please generate a new Q&A pair following the instructions. Only one question/answer pair should be generated per turn.'''\n",
    "    })\n",
    "    \n",
    "    if prev_examples:\n",
    "        for example in prev_examples:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": example})\n",
    "            messages.append({\"role\": \"user\", \"content\": 'Now, generate another unique question/answer pair.'})\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    content = resp.choices[0].message.content\n",
    "    \n",
    "    # Validate and return output\n",
    "    if '<question>' in content and '<answer>' in content:\n",
    "        return content.strip()\n",
    "    else:\n",
    "        raise ValueError(\"Generated content is not in the expected format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [05:05<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "for idx in tqdm(range(len(questions_to_revist))):\n",
    "    question = questions_to_revist[idx]\n",
    "    correct_answer = answers_of_interest[idx]\n",
    "    prev_examples = []\n",
    "    \n",
    "    for i in range(number_of_examples_per_question):\n",
    "        try:\n",
    "            example = generate_example(question, correct_answer, prev_examples, temperature)\n",
    "            prev_examples.append(example)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error generating example for question {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    for example in prev_examples:\n",
    "        try:\n",
    "            prompt_start = example.index('<question>') + len('<question>')\n",
    "            prompt_end = example.index('</question>')\n",
    "            prompt = example[prompt_start:prompt_end].strip()\n",
    "            \n",
    "            response_start = example.index('<answer>') + len('<answer>')\n",
    "            response_end = example.index('</answer>')\n",
    "            response = example[response_start:response_end].strip()\n",
    "            \n",
    "            prompts.append(prompt)\n",
    "            responses.append(response)\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Error parsing example: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 232 successfully-generated examples. Here are the first few:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statement 1| GPT-3 was trained on a dataset si...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Statement 1| BERT uses a masked language model...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statement 1| The T5 model was trained on a div...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statement 1| The XLNet model was trained using...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Statement 1| Decision trees, unlike support ve...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response\n",
       "0  Statement 1| GPT-3 was trained on a dataset si...        C\n",
       "1  Statement 1| BERT uses a masked language model...        A\n",
       "2  Statement 1| The T5 model was trained on a div...        C\n",
       "3  Statement 1| The XLNet model was trained using...        C\n",
       "4  Statement 1| Decision trees, unlike support ve...        D"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets, with 90% in the train set\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save the dataframes to .jsonl files\n",
    "train_df.to_json(f'{dataset_name}_train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json(f'{dataset_name}_test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
