{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A model that takes in a question in the field of 'professional medicine' and is given with a list of possible answers. The models produces an answer that is most likely to be correct. The model should just generate the answer and not provide any explanation or reasoning.\"\n",
    "temperature = .5\n",
    "number_of_examples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_example(prompt, prev_examples, temperature=0.5):\n",
    "    \n",
    "    system_prompt = f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\n<prompt>prompt</prompt>\\n<response>response_goes_here</response>\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
    "    \n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    }\n",
    "\n",
    "    messages = [system_message]\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": f'Now, generate a prompt/response pair for `{prompt}`. Do so in the exact format requested:\\n```\\n<prompt>prompt</prompt>\\n<response>response_goes_here</response>\\n```\\n\\nOnly one prompt/response pair should be generated per turn.'})\n",
    "    \n",
    "    if len(prev_examples) > 0:\n",
    "        if len(prev_examples) > 10:\n",
    "            prev_examples = random.sample(prev_examples, 10)\n",
    "\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": 'Now, generate another prompt/response pair. Make it unique.'\n",
    "            })\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content\n",
    "    \n",
    "    return '<prompt>' + content.split('<prompt>')[1]\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in tqdm(range(number_of_examples)):\n",
    "    example = generate_example(prompt, prev_examples, temperature)\n",
    "    prev_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system message is: `Given a question in the field of professional medicine and a list of possible answers, provide the most likely correct answer without any explanation or reasoning.`\n"
     ]
    }
   ],
   "source": [
    "def generate_system_message(prompt):\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"SYSTEM_PROMPT_HERE`.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Here is the prompt: `{prompt.strip()}`. Write a fantastic system message.\",\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "system_message = generate_system_message(prompt)\n",
    "\n",
    "print(f'The system message is: `{system_message}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 99 successfully-generated examples. Here are the first few:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary treatment for Type 1 Diabe...</td>\n",
       "      <td>A) Insulin therapy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which imaging modality is most commonly used t...</td>\n",
       "      <td>B) CT Pulmonary Angiography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the first-line medication for managing...</td>\n",
       "      <td>B) ACE inhibitors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which of the following vaccines is recommended...</td>\n",
       "      <td>C) Pneumococcal vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the management of chronic asthma, which cla...</td>\n",
       "      <td>C) Inhaled corticosteroids</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  What is the primary treatment for Type 1 Diabe...   \n",
       "1  Which imaging modality is most commonly used t...   \n",
       "2  What is the first-line medication for managing...   \n",
       "3  Which of the following vaccines is recommended...   \n",
       "4  In the management of chronic asthma, which cla...   \n",
       "\n",
       "                      response  \n",
       "0           A) Insulin therapy  \n",
       "1  B) CT Pulmonary Angiography  \n",
       "2            B) ACE inhibitors  \n",
       "3      C) Pneumococcal vaccine  \n",
       "4   C) Inhaled corticosteroids  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store prompts and responses\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "# Parse out prompts and responses from examples\n",
    "for example in prev_examples:\n",
    "    try:\n",
    "        prompt_start = example.index('<prompt>') + len('<prompt>')\n",
    "        prompt_end = example.index('</prompt>')\n",
    "        prompt = example[prompt_start:prompt_end].strip()\n",
    "\n",
    "        response_start = example.index('<response>') + len('<response>')\n",
    "        response_end = example.index('</response>')\n",
    "        response = example[response_start:response_end].strip()\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        responses.append(response)\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets, with 90% in the train set\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save the dataframes to .jsonl files\n",
    "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json('test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
